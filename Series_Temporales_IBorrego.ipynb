{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMsIAgMOYw+M/4gQH/ojD08",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/borre103/forecast-transformer/blob/main/Series_Temporales_IBorrego.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYzVE2xzWlph"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importación de librerías\n",
        "\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Configuración del estilo de los gráficos para mejorar visualización\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"notebook\", font_scale=1.2)"
      ],
      "metadata": {
        "id": "Su7cFAp_ZPXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la métrica MAPE (Mean Absolute Percentage Error), útil para evaluar errores relativos en modelos de predicción.\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Mean Absolute Percentage Error (MAPE)\n",
        "\n",
        "    Args:\n",
        "        y_true: Actual values\n",
        "        y_pred: Predicted values\n",
        "\n",
        "    Returns:\n",
        "        MAPE value\n",
        "    \"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # Se ignoran los valores donde y_true = 0 para evitar divisiones por cero\n",
        "    mask = y_true != 0\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100"
      ],
      "metadata": {
        "id": "122LQHW_aDaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "89pmZ4hyaEd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Carga y Limpieza inicial de los datos**\n",
        "\n",
        "Cargué una base de datos preprocesada con información mensual desde Octubre 2019 hasta Junio 2025 (no tenía información más antigua de importaciones), que incluye las ventas (en unidades) e importaciones de dos líneas de negocio: Ostomy Care (OC) e Intermittent Catheters (IC). También cuenta con la variable Lead_days, que representa el tiempo entre la importación y la disponibilidad de los productos en el país. Empezaré haciendo un forecast de las ventas de OC, y luego lo haré con IC\n",
        "\n",
        "En esta primera etapa, realicé un proceso de limpieza y validación de los datos. No conté con información histórica completa de Lead_days para todos los meses del dataset. Por eso, imputé los valores faltantes utilizando una estrategia que:\n",
        "\n",
        "\n",
        "*   Calcula el promedio observado de Lead_days (60.82 días)\n",
        "*   Genera valores aleatorios dentro del rango observado (mínimo: 41, máximo 97 días)\n",
        "*   Ajusta estos valores simulados para que el promedio total de la columna se mantenga lo más cercano posible a 60.82\n",
        "\n"
      ],
      "metadata": {
        "id": "OC1X5XE2M8zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leemos el archivo con separador explícito\n",
        "df = pd.read_csv(\"Database Transformers.csv\", sep=\";\", encoding=\"utf-8\", dtype={\"Fecha\":str})\n"
      ],
      "metadata": {
        "id": "jtnGAs5IhfX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminamos espacios de los nombres de columnas\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Columnas numéricas que deben limpiarse\n",
        "cols_to_clean = ['Ventas_OC', 'Ventas_IC', 'Importaciones_OC', 'Importaciones_IC']\n",
        "\n",
        "for col in cols_to_clean:\n",
        "    df[col] = df[col].astype(str)                         # Convertimos a string\n",
        "    df[col] = df[col].str.replace(\",\", \"\", regex=False)   # Sacamos comas como separador de miles\n",
        "    df[col] = df[col].str.strip().replace(['-', '', ' '], '0')  #Convertimos valores vacíos o inválidos en \"0\"\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)       # Volvemos a int (soporta NaN)\n",
        "\n",
        "\n",
        "df[\"Lead_days\"] = df[\"Lead_days\"].astype(float)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "odr7kZtpi-kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#El file me traia dos filas sin informacion con fecha N/A, Asi que voy a eliminarlas.\n",
        "df['Fecha'] = pd.to_datetime(df['Fecha'], format='%m/%d/%Y')\n"
      ],
      "metadata": {
        "id": "PjnQae8aVY3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hago una copia de las ventas originales en la columna Ventas_OC_raw antes de escalar, el cual va a ser clave para desescalar las predicciones\n",
        "df['Ventas_OC_raw'] = df['Ventas_OC']"
      ],
      "metadata": {
        "id": "ikZe1p01boK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Identifico cuántos valores faltan\n",
        "missing_count = df['Lead_days'].isna().sum()\n",
        "print(f\"Valores faltantes de Lead_days: {missing_count}\")\n",
        "\n",
        "# Valores conocidos\n",
        "lead_days_existentes = df['Lead_days'].dropna().values\n",
        "target_mean = 60.82\n",
        "\n",
        "# Paso 3: calcular suma total deseada\n",
        "total_values = len(df)\n",
        "suma_objetivo = total_values * target_mean\n",
        "suma_existente = lead_days_existentes.sum()\n",
        "suma_necesaria = suma_objetivo - suma_existente\n",
        "\n",
        "# Generó simulados dentro del rango 41-97 (min-max) que sumen lo necesario\n",
        "np.random.seed(42)\n",
        "# Inicializo con valores aleatorios\n",
        "lead_days_simulados = np.random.randint(41, 98, size=missing_count)\n",
        "\n",
        "# Ajusto proporcionalmente para alcanzar la suma necesaria\n",
        "factor = suma_necesaria / lead_days_simulados.sum()\n",
        "lead_days_simulados = np.clip((lead_days_simulados * factor).round(), 41, 97).astype(int)\n",
        "\n",
        "# insertar\n",
        "df.loc[df['Lead_days'].isna(), 'Lead_days'] = lead_days_simulados\n",
        "\n",
        "# Validación\n",
        "print(f\"Nuevo promedio de Lead_days: {df['Lead_days'].mean():.2f}\")\n",
        "print(f\"Min: {df['Lead_days'].min()}, Max: {df['Lead_days'].max()}\")\n"
      ],
      "metadata": {
        "id": "qydwoLCAZa8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.replace({pd.NA: np.nan})\n",
        "df = df.dropna()\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "sJ2C9BohtEIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MinMaxScaler transforma los valores de una columna numérica para que estén en un rango entre 0 y 1."
      ],
      "metadata": {
        "id": "lXf58La7QasZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler_ventas_oc = MinMaxScaler()\n",
        "#df['Ventas_OC_raw'] = df['Ventas_OC']  # Guardar original para comparar resultados de predicción vs realidad\n",
        "df['Ventas_OC'] = scaler_ventas_oc.fit_transform(df[['Ventas_OC']])\n"
      ],
      "metadata": {
        "id": "nesN4oZco2ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparación de las features**\n",
        "\n",
        "Creé un lag de 2 meses en importaciones y en el lead time, considerando que el tiempo promedio desde la importación hasta la venta es de aproximadamente 2 meses.\n",
        "\n",
        "También calculé un rolling mean de 3 meses para las ventas de OC para reducir la volatilidad.\n",
        "\n",
        "Como los lags y rolling requieren datos anteriores, las primeras filas generan NaN, por lo tanto fueron eliminadas.\n",
        "\n",
        "Luego, preparo el target: entreno un segundo MinMaxScaler solo en la columna Ventas_OC_raw para poder desescarlarlo tras la predicción\n"
      ],
      "metadata": {
        "id": "FtxwH9NN47tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear regresores con lag y rolling para OC e IC\n",
        "# Usamos lead_days promedio como referencia (~2 meses)\n",
        "\n",
        "# Lags (por lead time estimado)\n",
        "df['Importaciones_OC_lag2'] = df['Importaciones_OC'].shift(2)\n",
        "df['Importaciones_IC_lag2'] = df['Importaciones_IC'].shift(2)\n",
        "df['Lead_days_lag2'] = df['Lead_days'].shift(2)  # mismo para OC e IC\n",
        "\n",
        "# Rolling mean de ventas (estabiliza la serie)\n",
        "df['Ventas_OC_roll3'] = df['Ventas_OC'].rolling(window=3).mean()\n",
        "df['Ventas_IC_roll3'] = df['Ventas_IC'].rolling(window=3).mean()\n",
        "\n"
      ],
      "metadata": {
        "id": "7nAC3P6hhhQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminamos las primeras filas que quedaron con NaNs por los lags/rollings\n",
        "df = df.dropna().reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "D4yrC-K5wrPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_to_scale = [\n",
        "    'Importaciones_OC', 'Lead_days',\n",
        "    'Importaciones_OC_lag2', 'Lead_days_lag2', 'Ventas_OC_roll3'\n",
        "]\n",
        "\n",
        "scaler_features_oc = MinMaxScaler()\n",
        "df[features_to_scale] = scaler_features_oc.fit_transform(df[features_to_scale])\n",
        "\n",
        "scaler_target = MinMaxScaler()\n",
        "scaler_target.fit(df[['Ventas_OC_raw']])   # con la columna original de ventas\n"
      ],
      "metadata": {
        "id": "6kahzZzG6IXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definí las features y el objetivo. El modelo observará 12 meses para atrás e intentará predecir 3 meses en el futuro, donde la variable objetivo son las Ventas_OC.\n",
        "\n",
        "\n",
        "Creé un dataset personalizado (ForecastDataset) que genera muestras para el entrenamiento del modelo:\n",
        "\n",
        "\n",
        "*   x_demand: el target pasado (Histórico de ventas_oc de 12 meses)\n",
        "*   x_regressors: variables exógenas (Importaciones, lead days)\n",
        "*   y_demand: El objetivo (ventas de Julio a septiembre 2025)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "exjW4jp8Rd_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creo la columna target ya normalizada para el modelo\n",
        "df['target'] = scaler_target.transform(df[['Ventas_OC_raw']])\n",
        "#print(df[['Fecha', 'Ventas_OC_raw', 'target']].tail(10))\n"
      ],
      "metadata": {
        "id": "up3YyyJVcJZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora genero la columna target, que contiene las ventas originales escaladas entre 0 y 1, lista para entrenar el modelo. A continuación, renombro las columnas de lag para usar nombres más concisos en los features finales, y defino los parámetros de ventana input_len, forecast_horizon junto con la lista de variables de entradainput_features_OC."
      ],
      "metadata": {
        "id": "qVlYpIU76S0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rehacer input_features\n",
        "input_features_OC = [\n",
        "    'importaciones',\n",
        "    'lead_days',\n",
        "    'Ventas_OC_roll3'\n",
        "]\n",
        "\n",
        "target_col = 'target'\n",
        "input_len = 12\n",
        "forecast_horizon = 3\n",
        "\n",
        "df = df.rename(columns={\n",
        "    'Importaciones_OC_lag2': 'importaciones',\n",
        "    'Lead_days_lag2': 'lead_days'\n",
        "})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HRfA3ouWjCyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ForecastDataset(Dataset):\n",
        "    def __init__(self, df, input_window, forecast_window, input_features, target_col):\n",
        "        self.df = df\n",
        "        self.input_window = input_window\n",
        "        self.forecast_window = forecast_window\n",
        "        self.total_window = input_window + forecast_window\n",
        "        self.input_features = input_features\n",
        "        self.target_col = target_col\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) - self.total_window + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Input: target pasado (x_demand)\n",
        "        x_demand = torch.tensor(\n",
        "            self.df[self.target_col].iloc[idx : idx + self.input_window].values,\n",
        "            dtype=torch.float32\n",
        "        ).unsqueeze(-1)\n",
        "\n",
        "        # Input: features exógenas (x_regressors)\n",
        "        x_regressors = torch.tensor(\n",
        "            self.df[self.input_features].iloc[idx : idx + self.input_window].values,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # Output: target futuro (y_demand)\n",
        "        y_demand = torch.tensor(\n",
        "            self.df[self.target_col].iloc[idx + self.input_window : idx + self.total_window].values,\n",
        "            dtype=torch.float32\n",
        "        ).unsqueeze(-1)\n",
        "\n",
        "        return {\n",
        "            'x_demand': x_demand,\n",
        "            'x_regressors': x_regressors,\n",
        "            'y_demand': y_demand\n",
        "        }\n"
      ],
      "metadata": {
        "id": "VxDtHXWZsIRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_len = 12\n",
        "forecast_horizon = 3\n",
        "\n",
        "target_col = 'target'\n",
        "\n",
        "dataset = ForecastDataset(\n",
        "    df=df,\n",
        "    input_window=input_len,\n",
        "    forecast_window=forecast_horizon,\n",
        "    input_features=input_features_OC,\n",
        "    target_col='target'\n",
        ")"
      ],
      "metadata": {
        "id": "zz31SEa-soxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_item = dataset[0]\n",
        "x_demand_sample = sample_item['x_demand'].squeeze().numpy()\n"
      ],
      "metadata": {
        "id": "ny7BI3nVyeen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_item = dataset[0]\n",
        "\n",
        "x_demand_sample = sample_item['x_demand'].squeeze().numpy()\n",
        "y_demand_sample = sample_item['y_demand'].squeeze().numpy()\n",
        "\n",
        "x_indices = np.arange(len(x_demand_sample))\n",
        "y_indices = np.arange(len(x_demand_sample), len(x_demand_sample) + len(y_demand_sample))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(x_indices, x_demand_sample, label='X Demand (Input)', marker='o', linestyle='-')\n",
        "plt.plot(y_indices, y_demand_sample, label='Y Demand (Target)', marker='o', linestyle='-')\n",
        "plt.title('Sample X Demand and Y Demand from Dataset')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Demand (normalized)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UpaSguWmunt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Este bloque divide el dataset total en 3 subconjuntos:**\n",
        "\n",
        "\n",
        "*  80% para entrenamiento\n",
        "*  10% para validación\n",
        "*  10% para testeo\n",
        "\n",
        "Luego se crean Subset y DataLoader para cada uno, que sirven para alimentar el modelo por lotes"
      ],
      "metadata": {
        "id": "ZI9QfnIFTT0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# División 80% / 10% / 10%\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train_indices = list(range(0, train_size))\n",
        "val_indices = list(range(train_size, train_size + val_size))\n",
        "test_indices = list(range(train_size + val_size, total_size))\n",
        "\n",
        "# Subsets\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "9PpdnrFiz7tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnergyTimeSeriesDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_dataset, val_dataset, test_dataset, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        sample = train_dataset[0]\n",
        "        self.input_dim = sample['x_demand'].shape[1]\n",
        "        self.output_dim = sample['y_demand'].shape[1]\n",
        "        self.forecast_length = sample['y_demand'].shape[0]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def get_input_dim(self):\n",
        "        sample = self.train_dataset[0]\n",
        "        return sample['x_demand'].shape[1] + sample['x_regressors'].shape[1]\n",
        "\n",
        "    def get_output_dim(self):\n",
        "        return self.output_dim\n",
        "\n",
        "    def get_forecast_length(self):\n",
        "        return self.forecast_length\n"
      ],
      "metadata": {
        "id": "FmEZ-U6F2Pir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "uZnBu_jH8r-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo transformer para series temporales usando Pytorch Lightning"
      ],
      "metadata": {
        "id": "2opIxSkOUnBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class TransformerForecastingModel(pl.LightningModule):\n",
        "    def __init__(self, input_dim, output_dim, forecast_length, d_model=64, nhead=4, num_encoder_layers=2, dim_feedforward=128, dropout=0.1, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        self.decoder = nn.Linear(d_model, output_dim)\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, x_demand, x_regressors):\n",
        "        # Concatenar demanda + regresores si hay regresores\n",
        "        if x_regressors is not None:\n",
        "            x = torch.cat([x_demand, x_regressors], dim=-1)\n",
        "        else:\n",
        "            x = x_demand\n",
        "\n",
        "        # Pasar por input projection\n",
        "        x = self.input_projection(x)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        x = self.transformer_encoder(x)\n",
        "        output = self.decoder(x[:, -self.hparams.forecast_length:, :])\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        y_hat = self(batch['x_demand'], batch['x_regressors'])\n",
        "        loss = nn.functional.mse_loss(y_hat, batch['y_demand'])\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        y_hat = self(batch['x_demand'], batch['x_regressors'])\n",
        "        loss = nn.functional.mse_loss(y_hat, batch['y_demand'])\n",
        "        self.log('val_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        y_hat = self(batch['x_demand'], batch['x_regressors'])\n",
        "        loss = nn.functional.mse_loss(y_hat, batch['y_demand'])\n",
        "        self.log('test_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n"
      ],
      "metadata": {
        "id": "pyoJeFsz8hPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "class BaseForecaster(pl.LightningModule):\n",
        "    \"\"\"Base class for forecasting models with common functionality.\"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=1e-3):\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def common_step(self, batch):\n",
        "        \"\"\"Common operations for training, validation, and test steps.\"\"\"\n",
        "        x = batch['x_demand']\n",
        "        x_regressors = batch.get('x_regressors', None)\n",
        "        y = batch['y_demand']\n",
        "\n",
        "        y_hat = self(x, x_regressors)\n",
        "        loss = F.mse_loss(y_hat, y)\n",
        "        mae = F.l1_loss(y_hat, y)\n",
        "\n",
        "        # Calculate MAPE\n",
        "        y_np = y.cpu().numpy()\n",
        "        y_hat_np = y_hat.detach().cpu().numpy()\n",
        "        # Avoid division by zero\n",
        "        mask = y_np != 0\n",
        "        mape = np.mean(np.abs((y_np[mask] - y_hat_np[mask]) / y_np[mask])) * 100\n",
        "\n",
        "        return loss, mae, mape\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, mae, mape = self.common_step(batch)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, mae, mape = self.common_step(batch)\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_mae', mae)\n",
        "        self.log('val_mape', torch.tensor(mape))\n",
        "        return {'val_loss': loss, 'val_mae': mae, 'val_mape': mape}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, mae, mape = self.common_step(batch)\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_mae', mae)\n",
        "        self.log('test_mape', torch.tensor(mape))\n",
        "        return {'test_loss': loss, 'test_mae': mae, 'test_mape': mape}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n"
      ],
      "metadata": {
        "id": "LEmG_5s7AIDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerForecaster(BaseForecaster):\n",
        "    def __init__(self, input_dim, output_dim, forecast_length, hidden_dim=128, num_heads=4, num_layers=2, dropout=0.1, learning_rate=1e-3):\n",
        "        super().__init__(learning_rate)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.forecast_length = forecast_length\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 1000, hidden_dim))  # 1000: max sequence length\n",
        "\n",
        "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.output_projection = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x_demand, x_regressors=None):\n",
        "        if x_regressors is not None:\n",
        "            x = torch.cat([x_demand, x_regressors], dim=-1)\n",
        "        else:\n",
        "            x = x_demand\n",
        "\n",
        "        x = self.input_projection(x) + self.positional_encoding[:, :x.size(1), :]\n",
        "        decoder_input = torch.zeros(x.size(0), self.forecast_length, self.hidden_dim).to(x.device)\n",
        "        decoder_input = decoder_input + self.positional_encoding[:, :self.forecast_length, :]\n",
        "\n",
        "        out = self.transformer(x, decoder_input)\n",
        "        return self.output_projection(out)\n"
      ],
      "metadata": {
        "id": "y6yf3yD6AIpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este bloque de código configuro el proceso de entrenamiento usando PyTorch Lightning, que simplifica la gestión de checkpoint, early stopping y logging. Creo que la carpeta checkpoints para guardar los mejores pes. Creo que transformerforecaster pasandole las dimensiones input_dim, output_dim y forecastlength del DataModule, garantizando que coincidan con la forma de los datos. Tambien paso los hiperparámetros y defino una funcion get_trainer que guarda los mejores 3 modelos según val_loss y detiene el entrenamiento si val_loss no mejora en 10 epochs. Cargo los mejores pesos del mejor modelo entrenado y ejecuto el test para obtener métricas.\n",
        "\n"
      ],
      "metadata": {
        "id": "_wnc8GrTVvER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import pytorch_lightning as pl\n",
        "import os\n",
        "\n",
        "checkpoint_dir = \"/content/checkpoints/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# 1. Creé el DataModule\n",
        "data_module = EnergyTimeSeriesDataModule(\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# 2. Obtuve dimensiones desde el DataModule\n",
        "input_dim = data_module.get_input_dim()\n",
        "output_dim = data_module.get_output_dim()\n",
        "forecast_length = data_module.get_forecast_length()\n",
        "\n",
        "sample = train_dataset[0]\n",
        "\n",
        "\n",
        "output_dim = data_module.get_output_dim()\n",
        "forecast_length = data_module.get_forecast_length()\n",
        "# 3. Creé el modelo Transformer\n",
        "model = TransformerForecaster(\n",
        "    input_dim=input_dim,\n",
        "    output_dim=output_dim,\n",
        "    forecast_length=forecast_length,\n",
        "    hidden_dim=128,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    dropout=0.1,\n",
        "    learning_rate=1e-3\n",
        ")\n",
        "\n",
        "# 4. Definí la función para crear el trainer\n",
        "def get_trainer():\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor='val_loss',\n",
        "        dirpath= checkpoint_dir,\n",
        "        filename='transformer-{epoch:02d}-{val_loss:.2f}',\n",
        "        save_top_k=3,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    early_stop_callback = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        verbose=True,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=50,\n",
        "        callbacks=[checkpoint_callback, early_stop_callback],\n",
        "        log_every_n_steps=10,\n",
        "        enable_progress_bar=True\n",
        "    )\n",
        "    return trainer\n",
        "\n",
        "# 5. Creé el trainer\n",
        "trainer = get_trainer()\n",
        "\n",
        "# 6. Entrené el modelo\n",
        "trainer.fit(model, data_module)\n",
        "\n",
        "#Cargar el mejor checkpoint\n",
        "best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "print(\"Mejor modelo guardado en:\", best_model_path)\n",
        "\n",
        "best_model = TransformerForecaster.load_from_checkpoint(\n",
        "    checkpoint_path=best_model_path,\n",
        "    input_dim=input_dim,\n",
        "    output_dim=output_dim,\n",
        "    forecast_length=forecast_length,\n",
        "    hidden_dim=128,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    dropout=0.1,\n",
        "    learning_rate=1e-3\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# 7. Evalué en test set\n",
        "test_results = trainer.test(best_model, data_module)\n",
        "print(\"Resultados en el test set:\", test_results)"
      ],
      "metadata": {
        "id": "9pGOrWwsAI5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "test_loss = 0.05672543123364448\n",
        "rmse = sqrt(test_loss)\n",
        "print(f\"Transformer RMSE: {rmse:.4f}\")"
      ],
      "metadata": {
        "id": "A2CuSzvWBJVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfomer: MAE 0.1810, RMSE: 0.2382, y MAPE (%): 28.57%"
      ],
      "metadata": {
        "id": "WZupQ4bxn38_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con mis métricas base, voy a comparar con modelos tradicionales, empezando con Naive Forecasting. Lo que hace es simplemente replicar el último valor conocido como predicción para los próximos pasos."
      ],
      "metadata": {
        "id": "mE8zUbLzGX6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "# Crear listas para guardar predicciones y valores reales\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# forecast_length conocido (lo podemos obtener de un sample también)\n",
        "forecast_length = test_dataset[0]['y_demand'].shape[0]\n",
        "\n",
        "# Iterar sobre el test set\n",
        "for sample in test_dataset:\n",
        "    # Obtener la secuencia de entrada y salida\n",
        "    x_demand = sample['x_demand'].squeeze().numpy()  # (12,)\n",
        "    y_demand = sample['y_demand'].squeeze().numpy()  # (3,)\n",
        "\n",
        "    # Predicción naive: repetir el último valor de x_demand\n",
        "    last_value = x_demand[-1]\n",
        "    prediction = np.repeat(last_value, forecast_length)\n",
        "\n",
        "    # Guardar valores\n",
        "    y_true.append(y_demand)\n",
        "    y_pred.append(prediction)\n",
        "\n",
        "# Convertir a arrays\n",
        "y_true = np.array(y_true).reshape(-1)\n",
        "y_pred = np.array(y_pred).reshape(-1)\n",
        "\n",
        "# Calcular métricas\n",
        "naive_mae = mean_absolute_error(y_true, y_pred)\n",
        "naive_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "naive_mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100  # evitar división por cero\n",
        "\n",
        "print(f\"Naive MAE:  {naive_mae:.4f}\")\n",
        "print(f\"Naive RMSE: {naive_rmse:.4f}\")\n",
        "print(f\"Naive MAPE: {naive_mape:.2f}%\")"
      ],
      "metadata": {
        "id": "Ps2-e1GBBJMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mi modelo Transformer mejora al modelo Naive**\n",
        "\n",
        "\n",
        "*   Menor MAE: Mis predicciones están más cerca de los valores reales\n",
        "*   Bastante menor MAPE: el error porcentual relativo es casi la mitad, lo cual es muy positivo.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FBBtGhNVHBzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora voy a compararlo con el segundo modelo tradicional: Media Móvil Simple. Este método predice el valor futuro como promedio de los últimso N valores observados."
      ],
      "metadata": {
        "id": "xpVcmnMDHsVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "input_window = 12\n",
        "forecast_window = 3\n",
        "target_col = 'target'\n",
        "\n",
        "# Paso 1: Predicciones con media móvil\n",
        "sma_predictions = []\n",
        "for i in range(input_window, input_window + forecast_window):\n",
        "    prev_values = df[target_col].values[i - input_window : i]\n",
        "    sma_predictions.append(prev_values.mean())\n",
        "\n",
        "# Paso 2: Valores reales\n",
        "y_true = df[target_col].values[input_window : input_window + forecast_window]\n",
        "\n",
        "# Paso 3: Métricas\n",
        "sma_mae = mean_absolute_error(y_true, sma_predictions)\n",
        "sma_rmse = np.sqrt(mean_squared_error(y_true, sma_predictions))\n",
        "sma_mape = np.mean(np.abs((y_true - sma_predictions) / y_true)) * 100\n",
        "\n",
        "print(f\"SMA MAE:  {sma_mae:.4f}\")\n",
        "print(f\"SMA RMSE: {sma_rmse:.4f}\")\n",
        "print(f\"SMA MAPE: {sma_mape:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Zmkc36__AIUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resultados Mevia Móvil Simple\n",
        "\n",
        "A pesar de que la SMA logra el mejor error absoluto (MAE y RMSE), su MAPE es enorme (>145%)"
      ],
      "metadata": {
        "id": "GQ22OBADKZzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se compara con el modelo ARIMA, que se usa para pronosticar series temporales."
      ],
      "metadata": {
        "id": "TrprQhXYe1yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "\n",
        "# Paso 1: Parámetros y datos\n",
        "input_window = 12\n",
        "forecast_window = 3\n",
        "target_col = 'target'\n",
        "\n",
        "# Usamos los primeros input_window datos para entrenar el modelo\n",
        "train_series = df[target_col].iloc[:input_window]\n",
        "\n",
        "# Paso 2: Entrenar modelo ARIMA simple (sin estacionalidad)\n",
        "model_arima = ARIMA(train_series, order=(1,1,0))  # podés tunear los parámetros (p,d,q)\n",
        "model_fit = model_arima.fit()\n",
        "\n",
        "# Paso 3: Predecir los siguientes 3 valores\n",
        "forecast = model_fit.forecast(steps=forecast_window)\n",
        "\n",
        "# Paso 4: Valores reales\n",
        "y_true = df[target_col].iloc[input_window : input_window + forecast_window].values\n",
        "\n",
        "# Paso 5: Métricas\n",
        "arima_mae = mean_absolute_error(y_true, forecast)\n",
        "arima_rmse = np.sqrt(mean_squared_error(y_true, forecast))\n",
        "arima_mape = np.mean(np.abs((y_true - forecast) / y_true)) * 100\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"ARIMA MAE:  {arima_mae:.4f}\")\n",
        "print(f\"ARIMA RMSE: {arima_rmse:.4f}\")\n",
        "print(f\"ARIMA MAPE: {arima_mape:.2f}%\")\n"
      ],
      "metadata": {
        "id": "UuY3ErGQHq7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAE (Error Absoluto Medio): en promedio, el modelo se equivocó 0.1494 unidades.\n",
        "\n",
        "RMSE (Raíz del Error Cuadrático Medio): penaliza más los errores grandes. Acá fue 0.1796.\n",
        "\n",
        "MAPE (% de error absoluto medio): el error fue del 140.64% respecto al valor real, en promedio."
      ],
      "metadata": {
        "id": "xeZ2fGV-fBKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMForecaster(BaseForecaster):\n",
        "    def __init__(self, input_dim, output_dim, forecast_length, hidden_dim=128, learning_rate=1e-3):\n",
        "        super().__init__(learning_rate)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.forecast_length = forecast_length\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x_demand, x_regressors=None):\n",
        "        # Concatenar x_demand con x_regressors si existen\n",
        "        if x_regressors is not None:\n",
        "            x = torch.cat([x_demand, x_regressors], dim=-1)\n",
        "        else:\n",
        "            x = x_demand\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        last_hidden = lstm_out[:, -1, :]  # Tomamos la última salida de la secuencia\n",
        "\n",
        "        # Repetimos esa salida forecast_length veces para generar predicción multistep\n",
        "        repeated = last_hidden.unsqueeze(1).repeat(1, self.forecast_length, 1)\n",
        "        out = self.fc(repeated)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "4z80a9wCHq4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMForecaster(BaseForecaster):\n",
        "    def __init__(self, input_dim, output_dim, forecast_length, hidden_dim=128, learning_rate=1e-3):\n",
        "        super().__init__(learning_rate)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.forecast_length = forecast_length\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x_demand, x_regressors=None):\n",
        "        # Concatenar x_demand con x_regressors si existen\n",
        "        if x_regressors is not None:\n",
        "            x = torch.cat([x_demand, x_regressors], dim=-1)\n",
        "        else:\n",
        "            x = x_demand\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        last_hidden = lstm_out[:, -1, :]  # Tomamos la última salida de la secuencia\n",
        "\n",
        "        # Repetimos esa salida forecast_length veces para generar predicción multistep\n",
        "        repeated = last_hidden.unsqueeze(1).repeat(1, self.forecast_length, 1)\n",
        "        out = self.fc(repeated)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "yvfOuXebHqwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar el modelo LSTM\n",
        "\n",
        "\n",
        "model_lstm = LSTMForecaster(\n",
        "    input_dim=input_dim,     # mismo que usamos antes con +9 si tenías regresores\n",
        "    output_dim=output_dim,\n",
        "    forecast_length=forecast_length,\n",
        "    hidden_dim=128,\n",
        "    learning_rate=1e-3\n",
        ")\n",
        "\n",
        "# Entrenar\n",
        "trainer = get_trainer()\n",
        "trainer.fit(model_lstm, data_module)\n",
        "\n",
        "# Evaluar\n",
        "test_results_lstm = trainer.test(model_lstm, data_module)\n",
        "print(\"Resultados en el test set (LSTM):\", test_results_lstm)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M9QJ-eU9kPI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo                | MAE    | RMSE   | MAPE (%) |\n",
        "| --------------------- | ------ | ------ | -------- |\n",
        "| **Naive Forecasting** | 0.2464 | 0.3203 | 52.31    |\n",
        "| **SMA (Media Móvil)** | 0.1162 | 0.1387 | 145.78   |\n",
        "| **ARIMA**             | 0.1494 | 0.1796 | 140.65   |\n",
        "| **Transformer**       | 0.1810 | 0.2382 | 28.89    |\n",
        "| **LSTM**              | 0.2774 | 0.3550 | 41.24    |\n"
      ],
      "metadata": {
        "id": "25Hnmsq1ZHgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_loss = 0.12602275609970893\n",
        "RMSE = sqrt(test_loss)\n",
        "print(f\"LSTM RMSE: {RMSE:.4f}\")"
      ],
      "metadata": {
        "id": "UBM-bqLGkPHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el Modelo LSTM, el MAE es 0.2774, el RMSE 0.3550, el MAPE (%) 41.24. Por lo tanto, se evidencia que es mejor que ARIMA, con MAPE moderado."
      ],
      "metadata": {
        "id": "OEgdCvDNm2vV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión de comparación**\n",
        "\n",
        "*   En términos absolutos (MAE Y RMSE), la SMA es la que menor error presenta, seguida por ARIMA, el Transformer y finalmente el Naive.\n",
        "*   En términos relativos (MAPE), tanto SMA como ARIMA muestran errores porcentuales muy altos debido a meses de ventas bajas, lo que distorciona la métrica\n",
        "*   El Transformer consigue un equilibrio: ofrece un MAE y RMSE competitivos y un MAPE razonable (<30%), lo que indica robustez frente a variaciones bajas en la serie.\n",
        "\n",
        "Por lo tanto, para producción voy a utilizar el Transformer."
      ],
      "metadata": {
        "id": "Mqx3CdCT_hWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PREDICCIÓN DE MESES FUTUROS: JULIO, AGOSTO, SEPTIEMBRE\n",
        "\n",
        "#Preparar el DataFrame para predicción\n",
        "\n",
        "df_pred = df.copy()\n",
        "df_pred = df_pred.rename(columns={\n",
        "    'Ventas_OC': 'target',\n",
        "    'Importaciones_OC_lag2': 'importaciones',\n",
        "    'Lead_days_lag2': 'lead_days'\n",
        "})\n",
        "df_pred = df_pred.loc[:, ~df_pred.columns.duplicated()]\n",
        "\n",
        "df_pred[\"time_idx\"] = range(len(df_pred))\n",
        "\n",
        "input_features = ['importaciones', 'lead_days', 'Ventas_OC_roll3']\n",
        "target_col = 'target'\n",
        "input_len = 12\n",
        "forecast_len = 3\n",
        "\n",
        "df_pred[\"Ventas_OC_roll3\"] = df_pred[\"target\"].rolling(window=3, min_periods=1).mean()\n",
        "\n",
        "# Crear filas de futuro\n",
        "last_time_idx = df_pred[\"time_idx\"].max()\n",
        "last_date = df_pred[\"Fecha\"].max()\n",
        "future_dates = pd.date_range(start=last_date + pd.offsets.MonthBegin(1), periods=forecast_len, freq='MS')\n",
        "avg_importaciones = df_pred[\"importaciones\"].iloc[-3:].mean()\n",
        "avg_leaddays = df_pred[\"lead_days\"].iloc[-3:].mean()\n",
        "last_rolling = df_pred[\"Ventas_OC_roll3\"].iloc[-1]\n",
        "\n",
        "df_future = pd.DataFrame({\n",
        "    \"Fecha\": future_dates,\n",
        "    \"target\": [np.nan] * forecast_len,\n",
        "    \"importaciones\": [avg_importaciones] * forecast_len,\n",
        "    \"lead_days\": [avg_leaddays] * forecast_len,\n",
        "    \"Ventas_OC_roll3\": [last_rolling] * forecast_len,\n",
        "    \"time_idx\": range(last_time_idx + 1, last_time_idx + forecast_len + 1)\n",
        "})\n",
        "\n",
        "df_pred = pd.concat([df_pred, df_future], ignore_index=True)\n",
        "df_pred = df_pred.loc[:, ~df_pred.columns.duplicated()]\n",
        "df_pred = df_pred.reset_index(drop=True)\n",
        "\n",
        "\n",
        "df_pred[\"Ventas_OC_roll3\"] = df_pred[\"target\"].rolling(window=3, min_periods=1).mean()\n",
        "\n",
        "# Escalar features (solo usando histórico para fit) ---\n",
        "features_to_scale = ['importaciones', 'lead_days', 'Ventas_OC_roll3']\n",
        "df_hist = df_pred[df_pred[target_col].notna()].copy()\n",
        "\n",
        "scaler_features = MinMaxScaler()\n",
        "scaler_features.fit(df_hist[features_to_scale])\n",
        "scaler_target = MinMaxScaler()\n",
        "scaler_target.fit(df_hist[['Ventas_OC_raw']])\n",
        "\n",
        "for col in features_to_scale:\n",
        "    df_pred[col] = df_pred[col].fillna(df_pred[col].mean())\n",
        "df_pred[features_to_scale] = scaler_features.transform(df_pred[features_to_scale])\n",
        "\n",
        "# Preparar input de predicción (últimos input_len históricos) ---\n",
        "x_input = df_pred[input_features].iloc[-(forecast_len + input_len):-forecast_len].values\n",
        "x_input = torch.tensor(x_input, dtype=torch.float32).unsqueeze(0)  # (1, input_len, n_features)\n",
        "x_demand = df_pred[target_col].iloc[-(forecast_len + input_len):-forecast_len].values\n",
        "x_demand = torch.tensor(x_demand, dtype=torch.float32).view(1, input_len, 1)\n",
        "\n",
        "\n",
        "# 5. Predecir\n",
        "best_model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_scaled = best_model(x_demand, x_input)\n",
        "\n",
        "y_pred_scaled_np = y_pred_scaled.squeeze(0).cpu().numpy().reshape(-1, 1)\n",
        "y_pred_descaled = scaler_target.inverse_transform(y_pred_scaled_np)[:, 0]\n",
        "\n",
        "for fecha, pred in zip(future_dates, y_pred_descaled):\n",
        "    print(f\"➡️ {fecha.strftime('%B %Y')}: {pred:,.0f} unidades previstas\")\n"
      ],
      "metadata": {
        "id": "kSIDDtNgQW3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El Transformer entrenado con los útlimos 12 meses de datos estima que las ventas de Ostomy Care descenderán ligeramente de 741.543 unidades en Julio de 2025 a 737.150 unidades en Septiembre de 2025. Esta tendencia suavemente decreciente coincide con la estacionalidad observada en periodos de menor actividad final de fin de año fiscal. De todas maneras, sería recomendable considerar otras variables para que el modelo sea aún más robusto, como por ejemplo los niveles de stock (dato que no tengo)"
      ],
      "metadata": {
        "id": "5ycwQRJDAHOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot históricos\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df_pred['Fecha'], df_pred['Ventas_OC_raw'], label='Ventas históricas', marker='o')\n",
        "\n",
        "# Plot predicciones (solo meses futuros)\n",
        "plt.plot(future_dates, y_pred_descaled, label='Predicción Transformer', marker='o')\n",
        "\n",
        "plt.title('Ventas OC: Históricos y Predicción Transformer')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Unidades vendidas')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LRAsltqopyZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ahora haré lo mismo con IC**\n",
        "\n",
        "Realizo una preparación y un modelo para Intermittent Catheters.\n",
        "\n",
        "\n",
        "1.   Clono el DataFrame original y guardo la serie raw en Ventas_IC_raw\n",
        "2.   Los regresores con lags de importaciones y lead days ya fueron creados en al primera parte de la notebook\n",
        "3.   Escalo las features exógenas y luego el target con scalers entrenados.\n",
        "4.   Conformo los datasets y dataloaders con ventana de 12 meses y 3 meses de pronóstico (cierre de año fiscal)\n",
        "5.   Instancio y entreno el mismo transformer (solo cambiando el input_dim según IC)\n",
        "6.   Finalmente, desescalo las predicciones para devolverlas a unidades originales\n",
        "\n"
      ],
      "metadata": {
        "id": "sdzAbuZ4qgn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ic = df.copy(deep=True)\n",
        "# 2) Guardar raw\n",
        "df_ic['Ventas_IC_raw'] = df_ic['Ventas_IC']\n",
        "\n",
        "df_ic['Ventas_IC_roll3'] = df_ic['Ventas_IC_raw'].rolling(window=3, min_periods=1).mean()\n",
        "\n",
        "df_ic = df_ic.dropna().reset_index(drop=True)\n",
        "\n",
        "# 3) Escalar Ventas_IC como feature\n",
        "scaler_ventas_ic = MinMaxScaler()\n",
        "df_ic['Ventas_IC'] = scaler_ventas_ic.fit_transform(df_ic[['Ventas_IC']])\n",
        "\n",
        "if 'importaciones' in df_ic.columns:\n",
        "    df_ic = df_ic.drop(columns=['importaciones'])\n",
        "\n",
        "\n",
        "# 4) Renombrar lags\n",
        "df_ic = df_ic.rename(columns={\n",
        "    'Importaciones_IC_lag2': 'importaciones',\n",
        "    'Lead_days_lag2':        'lead_days'\n",
        "})\n",
        "\n",
        "# 5) Escalar resto de features\n",
        "features_to_scale_IC = [\n",
        "    'importaciones', 'lead_days', 'Ventas_IC_roll3'\n",
        "]\n",
        "scaler_features_ic = MinMaxScaler()\n",
        "df_ic[features_to_scale_IC] = scaler_features_ic.fit_transform(df_ic[features_to_scale_IC])\n",
        "\n",
        "# 6) Fit + transform para el target\n",
        "scaler_target_ic = MinMaxScaler()\n",
        "scaler_target_ic.fit(df_ic[['Ventas_IC_raw']])\n",
        "df_ic['target_IC'] = scaler_target_ic.transform(df_ic[['Ventas_IC_raw']])\n",
        "\n"
      ],
      "metadata": {
        "id": "xwgIkABkvAq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ni7blc84qlq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros de ventana\n",
        "input_len = 12\n",
        "forecast_horizon = 3\n",
        "\n",
        "\n",
        "# Columnas y DataFrame para IC\n",
        "# (asegúrate de que df_ic ya contiene 'target_IC' y las features escaladas)\n",
        "input_features_IC = ['importaciones', 'lead_days', 'Ventas_IC_roll3']\n",
        "target_col = 'target_IC'\n",
        "\n",
        "f_ic = df_ic.loc[:, ~df_ic.columns.duplicated()]\n",
        "expected_cols = ['importaciones', 'lead_days', 'Ventas_IC_roll3', 'target_IC']\n",
        "missing_cols = [col for col in expected_cols if col not in df_ic.columns]\n",
        "print(\"Faltan columnas:\", missing_cols)\n",
        "assert not missing_cols, f\"¡Faltan columnas clave para el modelo: {missing_cols}!\"\n",
        "\n",
        "\n",
        "# Crear dataset para IC\n",
        "dataset_ic = ForecastDataset(\n",
        "    df=df_ic,\n",
        "    input_window=input_len,\n",
        "    forecast_window=forecast_horizon,\n",
        "    input_features=input_features_IC,\n",
        "    target_col=target_col\n",
        ")\n",
        "\n",
        "# DEBUG: imprime las formas para asegurarnos\n",
        "sample = dataset_ic[0]\n",
        "print(\"x_demand shape    :\", sample['x_demand'].shape)\n",
        "print(\"x_regressors shape:\", sample['x_regressors'].shape)\n",
        "print(\"y_demand shape    :\", sample['y_demand'].shape)\n",
        "\n",
        "# Ejemplo de extracción de la primera muestra\n",
        "sample_item = dataset_ic[0]\n",
        "x_demand_sample = sample_item['x_demand'].squeeze().numpy()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wBHffmWyw974"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_size = len(dataset_ic)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train_indices = list(range(0, train_size))\n",
        "val_indices = list(range(train_size, train_size + val_size))\n",
        "test_indices = list(range(train_size + val_size, total_size))\n",
        "\n",
        "train_dataset = Subset(dataset_ic, train_indices)\n",
        "val_dataset = Subset(dataset_ic, val_indices)\n",
        "test_dataset = Subset(dataset_ic, test_indices)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "G8cF5C7yx1KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "HKT1gQjGtxF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# 1) Directorio para checkpoints\n",
        "checkpoint_dir = \"checkpoints_ic/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# 2) DataModule para IC (ya definido previamente)\n",
        "dm_ic = EnergyTimeSeriesDataModule(\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# 3) Obtener dimensiones desde el DataModule\n",
        "input_dim       = batch['x_demand'].shape[-1] + batch['x_regressors'].shape[-1]\n",
        "output_dim      = dm_ic.output_dim\n",
        "forecast_length = dm_ic.forecast_length\n",
        "\n",
        "# 4) Instanciar el modelo para IC\n",
        "model_ic = TransformerForecaster(\n",
        "    input_dim       = input_dim,\n",
        "    output_dim      = output_dim,\n",
        "    forecast_length = forecast_length,\n",
        "    hidden_dim      = 128,\n",
        "    num_heads       = 4,\n",
        "    num_layers      = 2,\n",
        "    dropout         = 0.1,\n",
        "    learning_rate   = 1e-3\n",
        ")\n",
        "\n",
        "# 5) Definir función que crea el Trainer\n",
        "def get_trainer_ic():\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor='val_loss',\n",
        "        dirpath=checkpoint_dir,\n",
        "        filename='transformer-IC-{epoch:02d}-{val_loss:.2f}',\n",
        "        save_top_k=3,\n",
        "        mode='min'\n",
        "    )\n",
        "    early_stop_callback = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        verbose=True,\n",
        "        mode='min'\n",
        "    )\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=50,\n",
        "        callbacks=[checkpoint_callback, early_stop_callback],\n",
        "        log_every_n_steps=10,\n",
        "        enable_progress_bar=True\n",
        "    )\n",
        "    return trainer\n",
        "\n",
        "# 6) Crear y usar el Trainer\n",
        "trainer_ic = get_trainer_ic()\n",
        "trainer_ic.fit(model_ic, dm_ic)\n",
        "\n",
        "# 7) Cargar el mejor checkpoint\n",
        "best_path_ic = trainer_ic.checkpoint_callback.best_model_path\n",
        "print(\"Mejor modelo guardado en:\", best_path_ic)\n",
        "best_model_ic = TransformerForecaster.load_from_checkpoint(\n",
        "    best_path_ic,\n",
        "    input_dim       = input_dim,\n",
        "    output_dim      = output_dim,\n",
        "    forecast_length = forecast_length,\n",
        "    hidden_dim      = 128,\n",
        "    num_heads       = 4,\n",
        "    num_layers      = 2,\n",
        "    dropout         = 0.1,\n",
        "    learning_rate   = 1e-3\n",
        ")\n",
        "\n",
        "# 8) Evaluar en el test set\n",
        "test_results_ic = trainer_ic.test(best_model_ic, datamodule=dm_ic)\n",
        "print(\"Resultados en el test set:\", test_results_ic)\n"
      ],
      "metadata": {
        "id": "B0ZuBeYm1QFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "test_loss_ic = 0.05672543123364448\n",
        "rmse = sqrt(test_loss_ic)\n",
        "print(f\"Transformer RMSE: {rmse:.4f}\")"
      ],
      "metadata": {
        "id": "V9QdfncZ2N8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo para IC reduce el MAE en un 34% respecto a OC, lo que indica que, en valores absolutos, las predicciones de IC son más precisas.\n",
        "\n",
        "El RMSE resulta idéntico para ambas líneas debido a que, numéricamente, el test_loss final de IC (≈0.0567) es muy similar al de OC.\n",
        "\n",
        "La mejora más notable está en el MAPE, que baja de 28.9% en OC a 15.6% en IC, reflejando que el Transformer captura mejor la dinámica relativa de IC, incluso en meses de baja demanda.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z04vsz69DNdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PREDICCIÓN DE MESES FUTUROS IC: Julio, Agosto, Septiembre\n",
        "\n",
        "# 1) Preparar el DataFrame para predicción\n",
        "df_pred_ic = df_ic.copy()\n",
        "\n",
        "# Renombrar el raw ‘Ventas_IC_raw’ a ‘target’\n",
        "df_pred_ic = df_pred_ic.rename(columns={'Ventas_IC_raw': 'target'})\n",
        "\n",
        "df_pred_ic['Ventas_IC_raw'] = df_ic['Ventas_IC_raw']\n",
        "\n",
        "# Asegurar que no haya columnas duplicadas\n",
        "df_pred_ic = df_pred_ic.loc[:, ~df_pred_ic.columns.duplicated()]\n",
        "\n",
        "# Índice de tiempo\n",
        "df_pred_ic['time_idx'] = range(len(df_pred_ic))\n",
        "\n",
        "# Volver a calcular rolling de ventana 3 sobre el target\n",
        "df_pred_ic['Ventas_IC_roll3'] = df_pred_ic['target'].rolling(window=3, min_periods=1).mean()\n",
        "\n",
        "# Parámetros\n",
        "input_features_ic = ['importaciones', 'lead_days', 'Ventas_IC_roll3']\n",
        "target_col        = 'target'\n",
        "input_len         = 12\n",
        "forecast_len      = 3\n",
        "\n",
        "# 2) Crear filas de futuro\n",
        "last_time_idx = df_pred_ic['time_idx'].max()\n",
        "last_date     = df_pred_ic['Fecha'].max()\n",
        "future_dates  = pd.date_range(start=last_date + pd.offsets.MonthBegin(1),\n",
        "                              periods=forecast_len, freq='MS')\n",
        "\n",
        "avg_import   = df_pred_ic['importaciones'].iloc[-3:].mean()\n",
        "avg_leaddays = df_pred_ic['lead_days'].iloc[-3:].mean()\n",
        "last_roll    = df_pred_ic['Ventas_IC_roll3'].iloc[-1]\n",
        "\n",
        "df_future_ic = pd.DataFrame({\n",
        "    'Fecha'           : future_dates,\n",
        "    'target'          : [np.nan] * forecast_len,\n",
        "    'importaciones'   : [avg_import] * forecast_len,\n",
        "    'lead_days'       : [avg_leaddays] * forecast_len,\n",
        "    'Ventas_IC_roll3' : [last_roll] * forecast_len,\n",
        "    'time_idx'        : range(last_time_idx + 1,\n",
        "                              last_time_idx + forecast_len + 1)\n",
        "})\n",
        "\n",
        "df_pred_ic = pd.concat([df_pred_ic, df_future_ic],\n",
        "                       ignore_index=True)\n",
        "df_pred_ic = df_pred_ic.loc[:, ~df_pred_ic.columns.duplicated()]\n",
        "df_pred_ic = df_pred_ic.reset_index(drop=True)\n",
        "\n",
        "# Recalcular rolling con los NaN al final\n",
        "df_pred_ic['Ventas_IC_roll3'] = df_pred_ic['target']\\\n",
        "    .rolling(window=3, min_periods=1).mean()\n",
        "\n",
        "# 3) Escalar features (fit sólo sobre histórico)\n",
        "features_to_scale_ic = ['importaciones', 'lead_days', 'Ventas_IC_roll3']\n",
        "df_hist_ic = df_pred_ic[df_pred_ic[target_col].notna()].copy()\n",
        "\n",
        "scaler_target_ic = MinMaxScaler()\n",
        "scaler_target_ic.fit(df_hist_ic[['Ventas_IC_raw']])\n",
        "\n",
        "\n",
        "# Rellenar NaN en regresores\n",
        "df_pred_ic[features_to_scale_ic] = df_pred_ic[features_to_scale_ic]\\\n",
        "    .fillna(df_pred_ic[features_to_scale_ic].mean())\n",
        "\n",
        "df_pred_ic[features_to_scale_ic] = scaler_features_ic.transform(\n",
        "    df_pred_ic[features_to_scale_ic]\n",
        ")\n",
        "\n",
        "# 4) Preparar tensores de entrada\n",
        "# Exógenos\n",
        "x_input_ic = df_pred_ic[input_features_ic]\\\n",
        "    .iloc[-(forecast_len + input_len):-forecast_len]\\\n",
        "    .values\n",
        "x_input_ic = torch.tensor(x_input_ic, dtype=torch.float32)\\\n",
        "    .unsqueeze(0)  # forma: (1, input_len, n_features)\n",
        "\n",
        "# Demanda pasada\n",
        "x_demand_ic = df_pred_ic[target_col]\\\n",
        "    .iloc[-(forecast_len + input_len):-forecast_len]\\\n",
        "    .values\n",
        "x_demand_ic = torch.tensor(x_demand_ic, dtype=torch.float32)\\\n",
        "    .view(1, input_len, 1)\n",
        "\n",
        "\n",
        "# 5) Predecir con el mejor modelo entrenado para IC\n",
        "best_model_ic.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_scaled_ic = best_model_ic(x_demand_ic, x_input_ic)\n",
        "\n",
        "y_pred_scaled_np = (y_pred_scaled_ic.squeeze(0)\n",
        "                    .cpu()\n",
        "                    .numpy()\n",
        "                    .reshape(-1, 1))\n",
        "y_pred_descaled_ic = scaler_target_ic.inverse_transform(y_pred_scaled_np)[:, 0]\n",
        "\n",
        "# Mostrar predicciones en unidades originales\n",
        "for fecha, pred in zip(future_dates, y_pred_descaled_ic):\n",
        "    print(f\"➡️ {fecha.strftime('%B %Y')}: {pred:,.0f} unidades previstas\")"
      ],
      "metadata": {
        "id": "z4Jii6N_2e8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot históricos\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df_pred_ic['Fecha'], df_pred_ic['Ventas_IC_raw'], label='Ventas históricas', marker='o')\n",
        "\n",
        "# Plot predicciones (solo meses futuros)\n",
        "plt.plot(future_dates, y_pred_descaled, label='Predicción Transformer', marker='o')\n",
        "\n",
        "plt.title('Ventas IC: Históricos y Predicción Transformer')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Unidades vendidas')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mHyLEwk0zLfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}